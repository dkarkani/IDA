{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read txt data files and convert them to proper csv files\n",
    "# txtFile: input filename (including directory if applicable)\n",
    "# csvFile: output filename (including directory if applicable)\n",
    "# vtabchar: vertical tab character in the original file (to be replaced with newline command '\\n')\n",
    "# delim: delimiter character used in the original file (to be replaced with comma)\n",
    "def txt2csv(txtFile, csvFile, vtabchar, delim):\n",
    "    with open(txtFile, 'r') as file:\n",
    "        data = file.read().replace(vtabchar, '\\n').replace(delim, ',')\n",
    "    with open(csvFile, 'w') as file:\n",
    "        file.write(data)    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the filename and location for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original filename and directory for txt data files\n",
    "K1DI2_txt = './Data/Komponente/Komponente_K1DI2.txt'\n",
    "K2LE1_txt = './Data/Komponente/Komponente_K2LE1.txt' \n",
    "K2LE2_txt = './Data/Komponente/Komponente_K2LE2.txt'\n",
    "K2ST1_txt = './Data/Komponente/Komponente_K2ST1.txt'\n",
    "K3AG2_txt = './Data/Komponente/Komponente_K3AG2.txt'\n",
    "K7_txt    = './Data/Komponente/Komponente_K7.txt'\n",
    "\n",
    "# converted txt filename and directory\n",
    "K1DI2_csv = './Data/Komponente/Komponente_K1DI2.csv'\n",
    "K2LE1_csv = './Data/Komponente/Komponente_K2LE1.csv'\n",
    "K2LE2_csv = './Data/Komponente/Komponente_K2LE2.csv'\n",
    "K2ST1_csv = './Data/Komponente/Komponente_K2ST1.csv'\n",
    "K3AG2_csv = './Data/Komponente/Komponente_K3AG2.csv'\n",
    "K7_csv    = './Data/Komponente/Komponente_K7.csv'\n",
    "\n",
    "# original filename and directory for csv data files\n",
    "# component data files\n",
    "K1BE1_csv = './Data/Komponente/Komponente_K1BE1.csv'\n",
    "K1BE2_csv = './Data/Komponente/Komponente_K1BE2.csv'\n",
    "K1DI1_csv = './Data/Komponente/Komponente_K1DI1.csv'\n",
    "K2ST2_csv = './Data/Komponente/Komponente_K2ST2.csv'\n",
    "K3AG1_csv = './Data/Komponente/Komponente_K3AG1.csv'\n",
    "K3SG1_csv = './Data/Komponente/Komponente_K3SG1.csv'\n",
    "K3SG2_csv = './Data/Komponente/Komponente_K3SG2.csv'\n",
    "K4_csv    = './Data/Komponente/Komponente_K4.csv'\n",
    "K5_csv    = './Data/Komponente/Komponente_K5.csv'\n",
    "K6_csv    = './Data/Komponente/Komponente_K6.csv'    \n",
    "\n",
    "# vehicle data files\n",
    "bestFahr1_11_csv = './Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM1_Typ11.csv'\n",
    "bestFahr1_12_csv = './Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM1_Typ12.csv'\n",
    "bestFahr2_21_csv = './Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM2_Typ21.csv'\n",
    "bestFahr2_22_csv = './Data/Fahrzeug/Bestandteile_Fahrzeuge_OEM2_Typ22.csv'\n",
    "\n",
    "fahr1_11_csv = './Data/Fahrzeug/Fahrzeuge_OEM1_Typ11.csv'\n",
    "fahr1_12_csv = './Data/Fahrzeug/Fahrzeuge_OEM1_Typ12.csv'\n",
    "fahr2_21_csv = './Data/Fahrzeug/Fahrzeuge_OEM2_Typ21.csv'\n",
    "fahr2_22_csv = './Data/Fahrzeug/Fahrzeuge_OEM2_Typ22.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and convert all the txt data files to csv\n",
    "txt2csv(K1DI2_txt, K1DI2_csv, '\t', '\\\\')\n",
    "txt2csv(K2LE1_txt, K2LE1_csv, '\u000b', 'II')\n",
    "txt2csv(K2LE2_txt, K2LE2_csv, '\u000b', '\\\\')\n",
    "txt2csv(K2ST1_txt, K2ST1_csv, '\u000b', '|')\n",
    "txt2csv(K3AG2_txt, K3AG2_csv, '\u000b', '\\\\')\n",
    "txt2csv(K7_txt   , K7_csv   , '\u000b', '\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data arrangements into 4 types, namely A, B, C, and D. \n",
    "# this separation is based on the column names of the datetime data type\n",
    "A = ['Fehlerhaft_Datum', 'origin']\n",
    "B = ['Produktionsdatum.x', 'Fehlerhaft_Datum.x', \n",
    "     'Produktionsdatum.y', 'Fehlerhaft_Datum.y']\n",
    "C = ['Produktionsdatum.x', 'Fehlerhaft_Datum.x', \n",
    "     'Produktionsdatum.y', 'Fehlerhaft_Datum.y', \n",
    "     'Produktionsdatum', 'Fehlerhaft_Datum']\n",
    "D = ['Produktionsdatum', 'Fehlerhaft_Datum']\n",
    "\n",
    "# set up a function to read the csv files\n",
    "def csvReader(csvFile, arr_type, delim=None):\n",
    "    if delim is not None:\n",
    "        dataset = pd.read_csv(csvFile, parse_dates=arr_type, \n",
    "                          low_memory=False, sep=delim)\n",
    "    else:\n",
    "        dataset = pd.read_csv(csvFile, parse_dates=arr_type, \n",
    "                          low_memory=False)\n",
    "    return dataset\n",
    "\n",
    "# read the converted csv files using the csvReader function\n",
    "K1DI2 = csvReader(K1DI2_csv, A)\n",
    "K2LE1 = csvReader(K2LE1_csv, B)\n",
    "K2LE2 = csvReader(K2LE2_csv, A)\n",
    "K2ST1 = csvReader(K2ST1_csv, D)\n",
    "K3AG2 = csvReader(K3AG2_csv, A)\n",
    "K7    = csvReader(K7_csv, A)\n",
    "\n",
    "# read the rest of the csv files using the csvReader function\n",
    "K1BE1 = csvReader(K1BE1_csv, A)\n",
    "K1BE2 = csvReader(K1BE2_csv, A, ';')\n",
    "K1DI1 = csvReader(K1DI1_csv, C)\n",
    "K2ST2 = csvReader(K2ST2_csv, A, ';')\n",
    "K3AG1 = csvReader(K3AG1_csv, C)\n",
    "K3SG1 = csvReader(K3SG1_csv, B)\n",
    "K3SG2 = csvReader(K3SG2_csv, A)\n",
    "K4    = csvReader(K4_csv,    B, ';')\n",
    "K5    = csvReader(K5_csv,    B)\n",
    "K6    = csvReader(K6_csv,    A, ';')\n",
    "\n",
    "# # read the vehicle data:\n",
    "# bestFahr1_11 = pd.read_csv(bestFahr1_11_csv)\n",
    "# bestFahr1_12 = pd.read_csv(bestFahr1_12_csv)\n",
    "# bestFahr2_21 = pd.read_csv(bestFahr2_21_csv, sep=';')\n",
    "# bestFahr2_22 = pd.read_csv(bestFahr2_22_csv, sep=';')\n",
    "# fahr1_11 = pd.read_csv(fahr1_11_csv, parse_dates=D, low_memory=False)\n",
    "# fahr1_12 = pd.read_csv(fahr1_12_csv, parse_dates=D, low_memory=False)\n",
    "# fahr2_21 = pd.read_csv(fahr2_21_csv, parse_dates=A, low_memory=False)\n",
    "# fahr2_22 = pd.read_csv(fahr2_22_csv, parse_dates=A, low_memory=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For datasets with data arrangements of type B and C, we need to consolidate the columns and eliminate the .x and .y suffixes. For type B, the tables are separated into 2, whereas for type C, the tables are separated into 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names to be renamed for type B\n",
    "col_names_x = {'Produktionsdatum.x':'Produktionsdatum', \n",
    "               'Herstellernummer.x':'Herstellernummer',\t\n",
    "               'Werksnummer.x':'Werksnummer',\n",
    "               'Fehlerhaft.x':'Fehlerhaft', \n",
    "               'Fehlerhaft_Datum.x':'Fehlerhaft_Datum',\n",
    "               'Fehlerhaft_Fahrleistung.x':'Fehlerhaft_Fahrleistung'}\n",
    "col_names_y = {'Produktionsdatum.y':'Produktionsdatum', \n",
    "               'Herstellernummer.y':'Herstellernummer',\t\n",
    "               'Werksnummer.y':'Werksnummer',\n",
    "               'Fehlerhaft.y':'Fehlerhaft', \n",
    "               'Fehlerhaft_Datum.y':'Fehlerhaft_Datum',\n",
    "               'Fehlerhaft_Fahrleistung.y':'Fehlerhaft_Fahrleistung'}\n",
    "\n",
    "# set up variables for the components. The components are engine (Motor), electrical components (Schaltung),\n",
    "# body components (Karosserie), and seats (Sitze)\n",
    "idMotor  = 'ID_Motor'\n",
    "idMotorx = 'ID_Motor.x'\n",
    "idMotory = 'ID_Motor.y'\n",
    "\n",
    "idSchalt  = 'ID_Schaltung'\n",
    "idSchaltx = 'ID_Schaltung.x'\n",
    "idSchalty = 'ID_Schaltung.y'\n",
    "\n",
    "idKaros  = 'ID_Karosserie'\n",
    "idKarosx = 'ID_Karosserie.x'\n",
    "idKarosy = 'ID_Karosserie.y'\n",
    "\n",
    "idSitze  = 'ID_Sitze'\n",
    "idSitzex = 'ID_Sitze.x' \n",
    "idSitzey = 'ID_Sitze.y'\n",
    "\n",
    "# separate type B tables based on the suffixes, rename the columns, and \n",
    "# concatenate vertically, and finally extract the columns that contain the data.\n",
    "# the cleaned up dataset is added with suffix '_c'\n",
    "def streamlineTypeB(dataset, colx, coly, ID, IDx, IDy):\n",
    "    dataset_x = dataset[dataset[IDx].notna()].rename(columns=colx)\n",
    "    dataset_y = dataset[dataset[IDy].notna()].rename(columns=coly)\n",
    "    dataset_x = dataset_x.rename(columns={IDx : ID})\n",
    "    dataset_y = dataset_y.rename(columns={IDy : ID})\n",
    "    dataset_x = dataset_x.loc[:, ID:'Fehlerhaft_Fahrleistung']\n",
    "    dataset_y = dataset_y.loc[:, ID:'Fehlerhaft_Fahrleistung']\n",
    "    dataset_c = pd.concat([dataset_x, dataset_y], axis=0).reset_index(drop=True)\n",
    "    return dataset_c\n",
    "\n",
    "K2LE1_c = streamlineTypeB(K2LE1, col_names_x, col_names_y, \n",
    "                            idSitze, idSitzex, idSitzey)\n",
    "K3SG1_c = streamlineTypeB(K3SG1, col_names_x, col_names_y, \n",
    "                            idSchalt, idSchaltx, idSchalty)\n",
    "K4_c    = streamlineTypeB(K4, col_names_x, col_names_y, \n",
    "                            idKaros, idKarosx, idKarosy)\n",
    "K5_c    = streamlineTypeB(K5, col_names_x, col_names_y, \n",
    "                            idKaros, idKarosx, idKarosy)\n",
    "\n",
    "# separate type C tables based on the suffixes, rename the columns, and \n",
    "# concatenate vertically, and finally extract the columns that contain the data.\n",
    "# the cleaned up dataset is added with suffix '_c'\n",
    "def streamlineTypeC(dataset, colx, coly, ID, IDx, IDy):\n",
    "    dataset_x = dataset[dataset[IDx].notna()].loc[:, IDx:'Fehlerhaft_Fahrleistung.x'].rename(columns=colx)\n",
    "    dataset_y = dataset[dataset[IDy].notna()].loc[:, IDy:'Fehlerhaft_Fahrleistung.y'].rename(columns=coly)\n",
    "    dataset_  = dataset[dataset[ID].notna()]\n",
    "    dataset_x = dataset_x.rename(columns={IDx : ID})\n",
    "    dataset_y = dataset_y.rename(columns={IDy : ID})\n",
    "    dataset_  = dataset_.loc[:, ID:'Fehlerhaft_Fahrleistung']    \n",
    "    dataset_c = pd.concat([dataset_x, dataset_y, dataset_], axis=0).reset_index(drop=True)\n",
    "    return dataset_c\n",
    "\n",
    "K1DI1_c = streamlineTypeC(K1DI1, col_names_x, col_names_y,\n",
    "                          idMotor, idMotorx, idMotory)\n",
    "K3AG1_c = streamlineTypeC(K3AG1, col_names_x, col_names_y,\n",
    "                          idSchalt, idSchaltx, idSchalty)\n",
    "K2ST1_c = K2ST1.loc[:, idSitze:'Fehlerhaft_Fahrleistung']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addProduktionsDatum(dataset):\n",
    "    dataset['Produktionsdatum'] = dataset['origin'] + pd.to_timedelta(dataset['Produktionsdatum_Origin_01011970'].astype('int'), unit='days')\n",
    "    return dataset\n",
    "\n",
    "def reformatDataset(dataset, ID):\n",
    "    dataset_c = pd.concat([dataset.loc[:,ID], \n",
    "                           dataset.loc[:,'Produktionsdatum'], \n",
    "                           dataset.loc[:,'Herstellernummer':'Fehlerhaft_Fahrleistung']], axis=1)\n",
    "    return dataset_c\n",
    "\n",
    "K1BE1_c = reformatDataset(addProduktionsDatum(K1BE1), idMotor)\n",
    "K1BE2_c = reformatDataset(addProduktionsDatum(K1BE2), idMotor)\n",
    "K1DI2_c = reformatDataset(addProduktionsDatum(K1DI2), idMotor)\n",
    "K2LE2_c = reformatDataset(addProduktionsDatum(K2LE2), idSitze)\n",
    "K2ST2_c = reformatDataset(addProduktionsDatum(K2ST2), idSitze)\n",
    "K3AG2_c = reformatDataset(addProduktionsDatum(K3AG2), idSchalt)\n",
    "K3SG2_c = reformatDataset(addProduktionsDatum(K3SG2), idSchalt)\n",
    "K6_c    = reformatDataset(addProduktionsDatum(K6), idKaros)\n",
    "K7_c    = reformatDataset(addProduktionsDatum(K7), idKaros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all the datasets clean and proper, we can start doing the case analysis. In this case study, the components produced between January 1st, 2011 and December 31st, 2015 are considered. \n",
    "\n",
    "First, we filter out the components that is not produced within the considered time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will extract the dataset within the specified time period\n",
    "def timePeriodFilter(dataset, ID):\n",
    "    startDate = '2011-01-01'\n",
    "    endDate   = '2015-12-31'\n",
    "    dataset_2011until2015 = pd.concat([dataset[(dataset['Produktionsdatum'] >= startDate) & \n",
    "                                       (dataset['Produktionsdatum'] <= endDate)].loc[:,[ID,'Produktionsdatum']],\n",
    "                                       dataset[(dataset['Produktionsdatum'] >= startDate) & \n",
    "                                       (dataset['Produktionsdatum'] <= endDate)].loc[:,'Herstellernummer':'Fehlerhaft_Fahrleistung']],\n",
    "                                       axis=1).reset_index(drop=True)\n",
    "    return dataset_2011until2015\n",
    "\n",
    "# create separate datasets for each components produced between the specified time period\n",
    "K1BE1_2011until2015 = timePeriodFilter(K1BE1_c, idMotor)\n",
    "K1BE2_2011until2015 = timePeriodFilter(K1BE2_c, idMotor)\n",
    "K1DI1_2011until2015 = timePeriodFilter(K1DI1_c, idMotor)\n",
    "K1DI2_2011until2015 = timePeriodFilter(K1DI2_c, idMotor)\n",
    "\n",
    "K2LE1_2011until2015 = timePeriodFilter(K2LE1_c, idSitze)\n",
    "K2LE2_2011until2015 = timePeriodFilter(K2LE2_c, idSitze)\n",
    "K2ST1_2011until2015 = timePeriodFilter(K2ST1_c, idSitze)\n",
    "K2ST2_2011until2015 = timePeriodFilter(K2ST2_c, idSitze)\n",
    "\n",
    "K3AG1_2011until2015 = timePeriodFilter(K3AG1_c, idSchalt)\n",
    "K3AG2_2011until2015 = timePeriodFilter(K3AG2_c, idSchalt)\n",
    "K3SG1_2011until2015 = timePeriodFilter(K3SG1_c, idSchalt)\n",
    "K3SG2_2011until2015 = timePeriodFilter(K3SG2_c, idSchalt)\n",
    "\n",
    "K4_2011until2015    = timePeriodFilter(K4_c, idKaros)\n",
    "K5_2011until2015    = timePeriodFilter(K5_c, idKaros)\n",
    "K6_2011until2015    = timePeriodFilter(K6_c, idKaros)\n",
    "K7_2011until2015    = timePeriodFilter(K7_c, idKaros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26368"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K1BE1_2011until2015[(K1BE1_2011until2015['Fehlerhaft'] == 1) & \n",
    "                    (K1BE1_2011until2015['Produktionsdatum'].dt.year == 2011)].loc[:,'Fehlerhaft'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
